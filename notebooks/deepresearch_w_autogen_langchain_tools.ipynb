{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bc3cfa",
   "metadata": {},
   "source": [
    "# ü§ñ Multi-Agent Deep Research System\n",
    "\n",
    "This notebook showcases how to harness the combined power of **AutoGen** and **LangChain** tools to automate and elevate deep research workflows. At its core, the system coordinates a network of specialized agents‚Äîeach executing a distinct role in the research and report generation workflow. Together, these agents collect data, analyze findings, and produce polished, insight-driven reports.\n",
    "\n",
    "[Open in Colab](https://colab.research.google.com/github/miztiik/taars/blob/master/notebooks/deepresearch_w_autogen_langchain_tools.ipynb) <a href=\"https://colab.research.google.com/github/miztiik/taars/blob/master/notebooks/deepresearch_w_autogen_langchain_tools.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## üß† Agent Roles\n",
    "\n",
    "- **üß≠ Planner**: Defines research scope, objectives, and success criteria\n",
    "- **üîç Researcher**: Gathers evidence using `wiki_search`, `web_search` and `web_fetch` for content extraction\n",
    "- **üß™ Critic**: Reviews plans and outputs for quality and completeness\n",
    "- **‚úçÔ∏è Editor**: Formats final reports with proper citations and structure\n",
    "\n",
    "## üîÑ System Architecture\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[User Query] --> B[üß≠ Planner]\n",
    "    B --> C[üîç Researcher]\n",
    "    C --> D[üß™ Critic]\n",
    "    D --> E[‚úçÔ∏è Editor]\n",
    "    E --> F[üìÑ Final Report]\n",
    "\n",
    "    B -.-> D\n",
    "    C -.-> B\n",
    "    D -.-> C\n",
    "\n",
    "    C --> G[Wiki Search]\n",
    "    C --> H[Web Search]\n",
    "    C --> I[Content Fetch]\n",
    "\n",
    "```\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Set environment variables:\n",
    "\n",
    "```bash\n",
    "# Required: Gemini API (or configure other models in cells below)\n",
    "export GEMINI_API_KEY=\"your_api_key\"\n",
    "export GEMINI_MODEL_NAME=\"gemini-1.5-flash\"\n",
    "export GEMINI_BASE_URL=\"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "```\n",
    "\n",
    "### Run Research Task\n",
    "\n",
    "1. **Execute all cells** in sequence\n",
    "2. **Modify the query** in the final cell:\n",
    "   ```python\n",
    "   output = asyncio.run(run_task(\"Your research question here\"))\n",
    "   ```\n",
    "3. **Monitor progress** in real-time through cell outputs\n",
    "\n",
    "## üìã Example Queries\n",
    "\n",
    "- **Financial Analysis**: `\"Indian steel sector growth prospects in an era of US tariffs\"`\n",
    "- **Economic Research**: `\"Government factors that improved Indian economy during Modi era\"`\n",
    "- **Tech Industry**: `\"Are we witnessing an AI infrastructure bubble? GDP investment vs productivity gains\"`\n",
    "- **AI/ML Trends**: `\"Current trends in tool usage during LLM training\"`\n",
    "\n",
    "## üìä Outputs & Monitoring\n",
    "\n",
    "| Output Type | Location                                          | Description                                            |\n",
    "| ----------- | ------------------------------------------------- | ------------------------------------------------------ |\n",
    "| **Reports** | `./reports/`                                      | Timestamped Markdown reports (auto-saved)              |\n",
    "| **Logs**    | `./logs/YYYYMMDD_HHMM_deep_research_agent.log`    | Timestamped execution logs with token usage            |\n",
    "| **State**   | `./YYYYMMDD_HHMM_[task_keywords]_team_state.json` | Task-specific conversation state for resume capability |\n",
    "\n",
    "## üîÑ **Resume Functionality**\n",
    "\n",
    "```python\n",
    "# Auto-resume from most recent state file\n",
    "await resume_from_saved_state()\n",
    "\n",
    "# Resume from specific state file\n",
    "await resume_from_saved_state(\"20250824_1430_green_hydrogen_viability_team_state.json\")\n",
    "\n",
    "# List all available state files\n",
    "list_team_state_files()\n",
    "```\n",
    "\n",
    "## üìù TODO & Roadmap\n",
    "\n",
    "- [ ] **Specialized Models**: Different LLMs for different agent roles (planning vs research vs writing)\n",
    "- [ ] **Semantic Depth Search**: Advanced content extraction with semantic similarity scoring\n",
    "- [ ] **Source Verification**: Cross-reference validation and fact-checking workflows\n",
    "- [ ] **Domain-Specific Tools**: Specialized research tools for finance, science, law, etc.\n",
    "- [ ] **Agent Control Flow Logging**: Meaningful event logging for agent-to-agent handoffs\n",
    "- [ ] **Organic Flow Orchestration**: Improved prompts for natural, adaptive conversation flow\n",
    "- [ ] **Streaming UI**: Real-time progress visualization and intervention capability\n",
    "- [ ] **Performance Metrics**: Research quality scoring and optimization analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "%pip install -qU ipykernel\n",
    "%pip install -qU loguru\n",
    "%pip install -qU python-dotenv\n",
    "\n",
    "%pip install -qU autogen-agentchat\n",
    "%pip install -qU autogen-ext\n",
    "%pip install -qU langchain\n",
    "%pip install -qU langchain-community\n",
    "%pip install -qU wikipedia\n",
    "%pip install -qU selenium unstructured\n",
    "%pip install -qU lxml\n",
    "%pip install -qU ddgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4222bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GOOGLE COLAB LINE WRAPPING\n",
    "# https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "get_ipython().events.register(\"pre_run_cell\", set_css)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %aimport -langchain_community\n",
    "# Automatically reload modules before executing code\n",
    "\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48776430",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERIC IMPORTS\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Annotated, Tuple\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nest_asyncio\n",
    "import tenacity\n",
    "from loguru import logger\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS\n",
    "\n",
    "## LOG CONFIG\n",
    "LOG_ROTATION_SIZE = \"10 MB\"\n",
    "LOG_RETENTION_DAYS = \"7 days\"\n",
    "\n",
    "## AUTOGEN CONFIG\n",
    "CONVERSATION_BUFFER_SIZE = 10\n",
    "TASK_TERMINATION_MAX_MESSAGES = 30\n",
    "API_CALL_DELAY_SECONDS = int(\n",
    "    os.environ.get(\"API_CALL_DELAY_SECONDS\", \"45\")\n",
    ")  # 45 seconds = 1.33 RPM\n",
    "NON_API_EVENT_DELAY = 0.5  # Small delay for non-API events\n",
    "\n",
    "## TOOL CONFIG\n",
    "WIKI_MAX_RESULTS = 5\n",
    "WIKI_MAX_CHARS = 5000\n",
    "WIKIPEDIA_MAX_DOCS = 2\n",
    "DDGS_MAX_RESULTS = 10\n",
    "WEB_CONTENT_MAX_LENGTH = 15000\n",
    "WEB_CONTENT_MIN_LENGTH = 50\n",
    "WEB_BATCH_MAX_URLS = 10\n",
    "SELENIUM_WINDOW_SIZE = \"1920,1080\"\n",
    "BROWSER_USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
    "TITLE_SLICE_LENGTH = 50\n",
    "TOP_RESULTS_COUNT = 3\n",
    "TASK_MAX_MEANINGFUL_WORDS = 3\n",
    "CRITIC_MAX_WORDS = 500\n",
    "URL_FETCH_DELAY = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING CONFIG\n",
    "notebook_dir = Path.cwd()\n",
    "log_dir = notebook_dir / \"logs\"\n",
    "\n",
    "# Generate timestamped log filename\n",
    "timestamp = datetime.now()\n",
    "log_timestamp = timestamp.strftime(\"%Y%m%d_%H%M\")\n",
    "log_file = log_dir / f\"{log_timestamp}_deep_research_agent.log\"\n",
    "\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if not getattr(sys, \"_loguru_configured\", False):\n",
    "    logger.remove()\n",
    "    logger.add(\n",
    "        str(log_file),\n",
    "        level=\"DEBUG\",\n",
    "        rotation=\"10 MB\",\n",
    "        retention=\"7 days\",\n",
    "        compression=\"zip\",\n",
    "        enqueue=True,\n",
    "    )\n",
    "    logger.add(\n",
    "        sys.stderr,\n",
    "        colorize=True,\n",
    "    )\n",
    "    sys._loguru_configured = True\n",
    "\n",
    "logger.info(f\"‚úÖ Logging configured successfully - Log file: {log_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.messages import StopMessage\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from ddgs import DDGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef193373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GEMINI MODEL CLIENT\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import ModelInfo\n",
    "\n",
    "load_dotenv(os.path.join(\"..\", \".env\"))\n",
    "\n",
    "# Confirm the API key is set\n",
    "assert os.environ[\"GEMINI_API_KEY\"], \"GEMINI_API_KEY is not set\"\n",
    "assert os.environ[\"GEMINI_MODEL_NAME\"], \"GEMINI_MODEL_NAME is not set\"\n",
    "assert os.environ[\"GEMINI_BASE_URL\"], \"GEMINI_BASE_URL is not set\"\n",
    "\n",
    "\n",
    "gemini_model_info = ModelInfo(\n",
    "    vision=False,\n",
    "    function_calling=True,\n",
    "    json_output=True,\n",
    "    family=os.environ[\"GEMINI_MODEL_NAME\"],\n",
    "    structured_output=True,\n",
    ")\n",
    "\n",
    "gemini_model_client = OpenAIChatCompletionClient(\n",
    "    model=os.environ[\"GEMINI_MODEL_NAME\"],\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    base_url=os.environ[\"GEMINI_BASE_URL\"],\n",
    "    model_info=gemini_model_info,\n",
    "    max_retries=2,\n",
    "    parallel_tool_calls=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30311843",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AZURE OPENAI MODEL CLIENT\n",
    "# from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "\n",
    "## Confirm the API key is set\n",
    "# assert os.environ[\"AZURE_OAI_DEPLOYMENT\"], \"AZURE_OAI_DEPLOYMENT is not set\"\n",
    "# assert os.environ[\"AZURE_OAI_MODEL_NAME\"], \"AZURE_OAI_MODEL_NAME is not set\"\n",
    "# assert os.environ[\"AZURE_OAI_MODEL_VERSION\"], \"AZURE_OAI_MODEL_VERSION is not set\"\n",
    "# assert os.environ[\"AZURE_OAI_BASE_URL\"], \"AZURE_OAI_BASE_URL is not set\"\n",
    "\n",
    "# az_oai_model_client = AzureOpenAIChatCompletionClient(\n",
    "#     azure_deployment=os.environ[\"AZURE_OAI_DEPLOYMENT\"],\n",
    "#     model=os.environ[\"AZURE_OAI_MODEL_NAME\"],\n",
    "#     api_version=os.environ[\"AZURE_OAI_MODEL_VERSION\"],\n",
    "#     azure_endpoint=os.environ[\"AZURE_OAI_BASE_URL\"],\n",
    "\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGENT TOOLS\n",
    "\n",
    "# Wiki Search Tool\n",
    "wiki_api = WikipediaAPIWrapper(\n",
    "    top_k_results=WIKI_MAX_RESULTS, doc_content_chars_max=WIKI_MAX_CHARS\n",
    ")\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
    "\n",
    "\n",
    "def wiki_full_search(input: str) -> str:\n",
    "    \"\"\"Search Wikipedia for a query and return maximum 2 results.\"\"\"\n",
    "    logger.info(f\"üîç wiki_full_search: Starting search for '{input}'\")\n",
    "\n",
    "    try:\n",
    "        search_docs = WikipediaLoader(\n",
    "            query=input, load_max_docs=WIKIPEDIA_MAX_DOCS\n",
    "        ).load()\n",
    "\n",
    "        if not search_docs:\n",
    "            logger.warning(f\"‚ö†Ô∏è wiki_full_search: No documents found for '{input}'\")\n",
    "            return f\"No Wikipedia articles found for query: {input}\"\n",
    "\n",
    "        logger.info(\n",
    "            f\"‚úÖ wiki_full_search: Found {len(search_docs)} documents for '{input}'\"\n",
    "        )\n",
    "\n",
    "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "            [\n",
    "                f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "                for doc in search_docs\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        content_length = len(formatted_search_docs)\n",
    "        logger.info(\n",
    "            f\"üìä wiki_full_search: Returning {content_length} characters for '{input}'\"\n",
    "        )\n",
    "\n",
    "        return formatted_search_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå wiki_full_search failed for '{input}': {str(e)}\")\n",
    "        return f\"Error searching Wikipedia for '{input}': {str(e)}\"\n",
    "\n",
    "\n",
    "def wiki_search(q: str) -> dict:\n",
    "    \"\"\"Return structured output including text and source.\"\"\"\n",
    "    logger.info(f\"üîç wiki_search: Starting search for '{q}'\")\n",
    "\n",
    "    try:\n",
    "        result = wiki_tool.run(q)\n",
    "\n",
    "        if not result or result.strip() == \"\":\n",
    "            logger.warning(f\"‚ö†Ô∏è wiki_search: Empty result for '{q}'\")\n",
    "            return {\"text\": \"No results found\", \"source\": \"Wikipedia\", \"query\": q}\n",
    "\n",
    "        result_length = len(result)\n",
    "        logger.info(f\"‚úÖ wiki_search: Retrieved {result_length} characters for '{q}'\")\n",
    "\n",
    "        return {\"text\": result, \"source\": \"Wikipedia\", \"query\": q}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå wiki_search failed for '{q}': {str(e)}\")\n",
    "        return {\"text\": f\"Error: {str(e)}\", \"source\": None, \"query\": q}\n",
    "\n",
    "\n",
    "# Web Search Tool\n",
    "def web_search(q: str) -> dict:\n",
    "    \"\"\"Search the web using DuckDuckGo for information.\"\"\"\n",
    "    logger.info(f\"üåê web_search: Starting web search for '{q}'\")\n",
    "\n",
    "    try:\n",
    "        results = DDGS().text(\n",
    "            q,\n",
    "            region=\"us-en\",\n",
    "            safesearch=\"off\",\n",
    "            max_results=DDGS_MAX_RESULTS,\n",
    "            backend=\"google, brave, bing, yahoo\",\n",
    "        )\n",
    "\n",
    "        if not results:\n",
    "            logger.warning(f\"‚ö†Ô∏è web_search: No results found for '{q}'\")\n",
    "            return {\n",
    "                \"text\": [],\n",
    "                \"source\": \"DuckDuckGo\",\n",
    "                \"query\": q,\n",
    "                \"results_found\": 0,\n",
    "            }\n",
    "\n",
    "        logger.info(f\"‚úÖ web_search: Found {len(results)} results for '{q}'\")\n",
    "\n",
    "        # Log sample of top results for debugging\n",
    "        if results:\n",
    "            top_titles = [\n",
    "                r.get(\"title\", \"No title\")[:TITLE_SLICE_LENGTH]\n",
    "                for r in results[:TOP_RESULTS_COUNT]\n",
    "            ]\n",
    "            logger.debug(\n",
    "                f\"üìã web_search: Logging {TOP_RESULTS_COUNT} results for '{q}': {top_titles}\"\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"text\": results,\n",
    "            \"source\": \"DuckDuckGo\",\n",
    "            \"query\": q,\n",
    "            \"results_found\": len(results),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå web_search failed for '{q}': {str(e)}\")\n",
    "        return {\"text\": f\"Error: {str(e)}\", \"source\": None, \"query\": q}\n",
    "\n",
    "\n",
    "def web_fetch(\n",
    "    url: str, max_content_length: int = WEB_CONTENT_MAX_LENGTH\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch web page content using Selenium for JavaScript-heavy sites.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to fetch content from\n",
    "        max_content_length: Maximum content length to return. Defaults to WEB_CONTENT_MAX_LENGTH\n",
    "\n",
    "    Returns:\n",
    "        Dict with keys: content, url, status, error (if any)\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"üåê web_fetch: Starting fetch for {url} (max_length: {max_content_length})\"\n",
    "    )\n",
    "\n",
    "    loader = None\n",
    "\n",
    "    try:\n",
    "        # Validate URL\n",
    "        if not url or not url.startswith((\"http://\", \"https://\")):\n",
    "            return {\n",
    "                \"content\": \"\",\n",
    "                \"url\": url,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"Invalid URL format\",\n",
    "            }\n",
    "\n",
    "        # Configure Selenium loader\n",
    "        loader = SeleniumURLLoader(\n",
    "            urls=[url],\n",
    "            continue_on_failure=True,\n",
    "            arguments=_get_selenium_arguments(),\n",
    "            browser=\"chrome\",\n",
    "        )\n",
    "\n",
    "        # Load content\n",
    "        logger.info(f\"üì• web_fetch: Loading content from {url}\")\n",
    "        documents = loader.load()\n",
    "\n",
    "        if not documents:\n",
    "            logger.warning(f\"‚ö†Ô∏è web_fetch: No documents loaded from {url}\")\n",
    "            return {\n",
    "                \"content\": \"\",\n",
    "                \"url\": url,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"No content could be loaded from URL\",\n",
    "            }\n",
    "\n",
    "        # Process content\n",
    "        content = documents[0].page_content.strip()\n",
    "        original_length = len(content)\n",
    "\n",
    "        logger.debug(f\"üìä web_fetch: Loaded {original_length} characters from {url}\")\n",
    "\n",
    "        if len(content) < WEB_CONTENT_MIN_LENGTH:\n",
    "            logger.warning(\n",
    "                f\"‚ö†Ô∏è web_fetch: Content too short ({len(content)} chars) from {url}\"\n",
    "            )\n",
    "            return {\n",
    "                \"content\": content,\n",
    "                \"url\": url,\n",
    "                \"status\": \"warning\",\n",
    "                \"error\": \"Content appears too short, may indicate loading issues\",\n",
    "            }\n",
    "\n",
    "        # Truncate if too long\n",
    "        if len(content) > max_content_length:\n",
    "            content = content[:max_content_length] + \"\\n\\n[Content truncated...]\"\n",
    "            logger.info(\n",
    "                f\"‚úÇÔ∏è web_fetch: Content truncated from {original_length} to {max_content_length} chars for {url}\"\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f\"‚úÖ web_fetch: Successfully fetched {len(content)} characters from {url}\"\n",
    "        )\n",
    "\n",
    "        return {\"content\": content, \"url\": url, \"status\": \"success\", \"error\": None}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå web_fetch failed for {url}: {str(e)}\")\n",
    "        return {\n",
    "            \"content\": \"\",\n",
    "            \"url\": url,\n",
    "            \"status\": \"error\",\n",
    "            \"error\": f\"Failed to fetch content: {str(e)}\",\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # CRITICAL: Clean up browser resources\n",
    "        if loader and hasattr(loader, \"web_driver\") and loader.web_driver:\n",
    "            try:\n",
    "                loader.web_driver.quit()\n",
    "                logger.debug(f\"üßπ web_fetch: Browser cleaned up for {url}\")\n",
    "            except Exception as cleanup_error:\n",
    "                logger.warning(f\"‚ö†Ô∏è web_fetch: Browser cleanup failed: {cleanup_error}\")\n",
    "\n",
    "\n",
    "def _get_selenium_arguments() -> List[str]:\n",
    "    \"\"\"Get optimized Selenium browser arguments for reliability and stealth.\n",
    "    Returns:\n",
    "        List of browser arguments\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Core stability\n",
    "        \"--headless\",\n",
    "        \"--no-sandbox\",\n",
    "        \"--disable-dev-shm-usage\",\n",
    "        \"--disable-gpu\",\n",
    "        f\"--window-size={SELENIUM_WINDOW_SIZE}\",\n",
    "        # Performance\n",
    "        \"--disable-extensions\",\n",
    "        \"--disable-plugins\",\n",
    "        \"--disable-images\",\n",
    "        \"--disable-javascript\",\n",
    "        # Stealth and compatibility\n",
    "        \"--disable-blink-features=AutomationControlled\",\n",
    "        f\"--user-agent={BROWSER_USER_AGENT}\",\n",
    "        # GDPR/Cookie banner handling\n",
    "        \"--disable-notifications\",\n",
    "        \"--disable-infobars\",\n",
    "        \"--disable-default-apps\",\n",
    "        # Security bypasses (use cautiously)\n",
    "        \"--ignore-certificate-errors\",\n",
    "        \"--ignore-ssl-errors\",\n",
    "        \"--allow-running-insecure-content\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def web_fetch_multiple(\n",
    "    urls: List[str], max_content_length: int = WEB_CONTENT_MAX_LENGTH\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch content from multiple URLs efficiently.\n",
    "\n",
    "    Args:\n",
    "        urls: List of URLs to fetch\n",
    "        max_content_length: Maximum content length per URL\n",
    "\n",
    "    Returns:\n",
    "        Dict with results for each URL and summary statistics\n",
    "    \"\"\"\n",
    "    logger.info(f\"üåê web_fetch_multiple: Starting batch fetch for {len(urls)} URLs\")\n",
    "\n",
    "    if not urls or len(urls) > WEB_BATCH_MAX_URLS:\n",
    "        logger.warning(\n",
    "            f\"‚ö†Ô∏è web_fetch_multiple: Invalid URL list - {len(urls) if urls else 0} URLs (max {WEB_BATCH_MAX_URLS})\"\n",
    "        )\n",
    "        return {\n",
    "            \"results\": [],\n",
    "            \"status\": \"error\",\n",
    "            \"error\": f\"Invalid URL list (empty or too many URLs, max {WEB_BATCH_MAX_URLS})\",\n",
    "        }\n",
    "\n",
    "    results = []\n",
    "    success_count = 0\n",
    "\n",
    "    logger.debug(\n",
    "        f\"üìã web_fetch_multiple: Processing URLs: {[f'{url[:TITLE_SLICE_LENGTH]}...' if len(url) > TITLE_SLICE_LENGTH else url for url in urls]}\"\n",
    "    )\n",
    "\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        logger.debug(f\"üîÑ web_fetch_multiple: Processing URL {i}/{len(urls)}: {url}\")\n",
    "\n",
    "        result = web_fetch(url, max_content_length)\n",
    "        results.append(result)\n",
    "\n",
    "        if result[\"status\"] == \"success\":\n",
    "            success_count += 1\n",
    "            logger.debug(f\"‚úÖ web_fetch_multiple: URL {i}/{len(urls)} successful\")\n",
    "        else:\n",
    "            logger.debug(\n",
    "                f\"‚ùå web_fetch_multiple: URL {i}/{len(urls)} failed: {result.get('error', 'Unknown error')}\"\n",
    "            )\n",
    "\n",
    "        # Brief delay to be respectful\n",
    "        time.sleep(URL_FETCH_DELAY)\n",
    "\n",
    "    logger.info(\n",
    "        f\"üèÅ web_fetch_multiple: Completed batch - {success_count}/{len(urls)} successful\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"total_urls\": len(urls),\n",
    "        \"successful\": success_count,\n",
    "        \"failed\": len(urls) - success_count,\n",
    "        \"status\": \"completed\",\n",
    "    }\n",
    "\n",
    "\n",
    "def save_report(\n",
    "    content: str, task_description: str, reports_dir: str = \"reports\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Save timestamped Markdown report to disk with auto-generated filename.\n",
    "\n",
    "    Args:\n",
    "        content: Report content (plain text or Markdown). Auto-adds title if missing.\n",
    "        task_description: Brief task description for filename generation.\n",
    "        reports_dir: Output directory (default: \"reports\"). Created if missing.\n",
    "\n",
    "    Returns:\n",
    "        Dict with keys: status (\"success\"/\"error\"), filepath, filename, error\n",
    "\n",
    "    Examples:\n",
    "        save_report(\"# Analysis\\n\\nFindings...\", \"market research 2024\")\n",
    "        # ‚Üí reports/20250824_1430_15_market_research.md\n",
    "\n",
    "        save_report(draft_text, \"AI impact assessment\")\n",
    "        # ‚Üí reports/20250824_1431_22_ai_impact.md\n",
    "\n",
    "    Filename: YYYYMMDD_HHMM_SS_key_words.md (auto-numbered if exists)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure reports directory exists\n",
    "        reports_path = Path(reports_dir)\n",
    "        reports_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Generate timestamped filename with seconds for better uniqueness\n",
    "        timestamp = datetime.now()\n",
    "        date_time = timestamp.strftime(\"%Y%m%d_%H%M_%S\")\n",
    "        task_name = _extract_task_name(task_description)\n",
    "\n",
    "        filename = f\"{date_time}_{task_name}.md\"\n",
    "        filepath = reports_path / filename\n",
    "\n",
    "        # Handle filename conflicts with counter\n",
    "        counter = 1\n",
    "        while filepath.exists():\n",
    "            filename = f\"{date_time}_{task_name}_{counter}.md\"\n",
    "            filepath = reports_path / filename\n",
    "            counter += 1\n",
    "\n",
    "        # Format content with title if needed\n",
    "        formatted_content = _format_content(content, task_description, timestamp)\n",
    "\n",
    "        # Save to disk using Path object\n",
    "        filepath.write_text(formatted_content, encoding=\"utf-8\")\n",
    "        logger.info(f\"üìÑ Report saved: {filepath}\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"filepath\": str(filepath),\n",
    "            \"filename\": filename,\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to save report: {str(e)}\"\n",
    "        logger.error(f\"‚ùå {error_msg}\")\n",
    "        return {\"status\": \"error\", \"error\": error_msg, \"task\": task_description}\n",
    "\n",
    "\n",
    "def _extract_task_name(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract 2-3 meaningful words from task description for filename.\n",
    "\n",
    "    Args:\n",
    "        task: The task description string\n",
    "\n",
    "    Returns:\n",
    "        Underscore-separated words suitable for filename\n",
    "    \"\"\"\n",
    "    # Clean special characters and normalize\n",
    "    clean_task = re.sub(r\"[^\\w\\s]\", \" \", task.lower())\n",
    "    words = [w for w in clean_task.split() if len(w) > 2]\n",
    "\n",
    "    # Filter common stop words\n",
    "    stop_words = {\n",
    "        \"the\",\n",
    "        \"and\",\n",
    "        \"for\",\n",
    "        \"with\",\n",
    "        \"from\",\n",
    "        \"about\",\n",
    "        \"into\",\n",
    "        \"through\",\n",
    "        \"during\",\n",
    "        \"before\",\n",
    "        \"after\",\n",
    "        \"above\",\n",
    "        \"below\",\n",
    "        \"over\",\n",
    "        \"under\",\n",
    "    }\n",
    "    meaningful = [w for w in words if w not in stop_words][:TASK_MAX_MEANINGFUL_WORDS]\n",
    "\n",
    "    return \"_\".join(meaningful) if meaningful else \"report\"\n",
    "\n",
    "\n",
    "def _format_content(content: str, task: str, timestamp: datetime) -> str:\n",
    "    \"\"\"\n",
    "    Format content with title and timestamp if needed.\n",
    "\n",
    "    Args:\n",
    "        content: Raw content to format\n",
    "        task: Task description for title generation\n",
    "        timestamp: When the report was created\n",
    "\n",
    "    Returns:\n",
    "        Formatted Markdown content\n",
    "    \"\"\"\n",
    "    # If content already has a Markdown title, use as-is\n",
    "    if content.strip().startswith(\"#\"):\n",
    "        return content\n",
    "\n",
    "    # Add title and timestamp for plain text content\n",
    "    formatted_title = f\"# Report: {task}\"\n",
    "    timestamp_line = f\"*Generated: {timestamp.strftime('%Y-%m-%d %H:%M')}*\"\n",
    "\n",
    "    return f\"{formatted_title}\\n\\n{timestamp_line}\\n\\n{content}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REGISTER FUNCTIONS AS TOOLS\n",
    "\n",
    "wiki_search_tool = FunctionTool(\n",
    "    func=wiki_search,\n",
    "    name=\"wiki_search\",\n",
    "    description=\"Search Wikipedia for information.\",\n",
    ")\n",
    "web_search_tool = FunctionTool(\n",
    "    func=web_search, name=\"web_search\", description=\"Search the web for information.\"\n",
    ")\n",
    "web_fetch_tool = FunctionTool(\n",
    "    func=web_fetch, name=\"web_fetch\", description=\"Fetch content from a web page.\"\n",
    ")\n",
    "web_fetch_multiple_tool = FunctionTool(\n",
    "    func=web_fetch_multiple,\n",
    "    name=\"web_fetch_multiple\",\n",
    "    description=\"Fetch content from multiple web pages.\",\n",
    ")\n",
    "save_report_tool = FunctionTool(\n",
    "    func=save_report, name=\"save_report\", description=\"Save the research report.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b446dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UTILITY FUNCTIONS\n",
    "\n",
    "# Global token accounting - managed by log_event_enhanced\n",
    "_token_accounting = {\n",
    "    \"total_prompt_tokens\": 0,\n",
    "    \"total_completion_tokens\": 0,\n",
    "    \"total_tokens\": 0,\n",
    "    \"llm_call_count\": 0,\n",
    "    \"processed_event_ids\": set(),\n",
    "}\n",
    "\n",
    "\n",
    "def reset_token_accounting():\n",
    "    \"\"\"Reset global token accounting for new task.\"\"\"\n",
    "    global _token_accounting\n",
    "    _token_accounting = {\n",
    "        \"total_prompt_tokens\": 0,\n",
    "        \"total_completion_tokens\": 0,\n",
    "        \"total_tokens\": 0,\n",
    "        \"llm_call_count\": 0,\n",
    "        \"processed_event_ids\": set(),\n",
    "    }\n",
    "    logger.debug(\"üîÑ Token accounting reset for new task\")\n",
    "\n",
    "\n",
    "def get_token_stats():\n",
    "    \"\"\"Get current token statistics.\"\"\"\n",
    "    return _token_accounting.copy()\n",
    "\n",
    "\n",
    "def log_event_enhanced(\n",
    "    event,\n",
    "    event_count: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced agent-aware logging with centralized token accounting and LLM call tracking.\n",
    "\n",
    "    This function now handles ALL token counting and LLM call tracking logic,\n",
    "    keeping the run_task function clean and simple.\n",
    "\n",
    "    Args:\n",
    "        event: The event object from the team stream\n",
    "        event_count: Total number of events processed\n",
    "\n",
    "    Returns:\n",
    "        bool: True if this was an LLM API call, False otherwise\n",
    "    \"\"\"\n",
    "    global _token_accounting\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    event_id = getattr(event, \"id\", f\"event_{event_count}\")\n",
    "    is_llm_call = False\n",
    "\n",
    "    # Agent-specific emojis and formatting\n",
    "    agent_config = {\n",
    "        \"Planner\": {\"emoji\": \"üß≠\", \"role\": \"PLANNER\"},\n",
    "        \"Researcher\": {\"emoji\": \"üîç\", \"role\": \"RESEARCHER\"},\n",
    "        \"Critic\": {\"emoji\": \"‚öñÔ∏è\", \"role\": \"CRITIC\"},\n",
    "        \"Editor\": {\"emoji\": \"‚úçÔ∏è\", \"role\": \"EDITOR\"},\n",
    "    }\n",
    "\n",
    "    # Get agent info\n",
    "    source = getattr(event, \"source\", \"Unknown\")\n",
    "    event_type = getattr(event, \"type\", type(event).__name__)\n",
    "    config = agent_config.get(source, {\"emoji\": \"ü§ñ\", \"role\": \"UNKNOWN\"})\n",
    "\n",
    "    # CENTRALIZED TOKEN ACCOUNTING - only count once per unique event\n",
    "    current_prompt = 0\n",
    "    current_completion = 0\n",
    "    current_total = 0\n",
    "\n",
    "    if (\n",
    "        hasattr(event, \"models_usage\")\n",
    "        and event.models_usage\n",
    "        and event_id not in _token_accounting[\"processed_event_ids\"]\n",
    "    ):\n",
    "        current_prompt = event.models_usage.prompt_tokens\n",
    "        current_completion = event.models_usage.completion_tokens\n",
    "        current_total = current_prompt + current_completion\n",
    "\n",
    "        # Update global running totals ONLY for new events\n",
    "        _token_accounting[\"total_prompt_tokens\"] += current_prompt\n",
    "        _token_accounting[\"total_completion_tokens\"] += current_completion\n",
    "        _token_accounting[\"total_tokens\"] += current_total\n",
    "\n",
    "        # Mark this event as processed\n",
    "        _token_accounting[\"processed_event_ids\"].add(event_id)\n",
    "\n",
    "        # This is an actual LLM API call\n",
    "        _token_accounting[\"llm_call_count\"] += 1\n",
    "        is_llm_call = True\n",
    "\n",
    "        logger.debug(\n",
    "            f\"üí∞ Token accounting: Event {event_id} - prompt:{current_prompt}, completion:{current_completion}, total:{current_total}\"\n",
    "        )\n",
    "\n",
    "    # Build the enhanced structure with clear LLM call vs total event distinction\n",
    "    call_info = (\n",
    "        f\"LLM Call #{_token_accounting['llm_call_count']}\"\n",
    "        if is_llm_call\n",
    "        else \"Non-LLM Event\"\n",
    "    )\n",
    "    line1 = f\"Agent: {config['emoji']} {config['role']} | Type: {event_type} | {call_info} | Total Events: #{event_count}\"\n",
    "    line2 = \"\"  # Reserved for future functional info\n",
    "\n",
    "    if is_llm_call and current_total > 0:\n",
    "        # Show current event tokens + running totals for LLM calls\n",
    "        line3 = f\"Tokens: Current[prompt:{current_prompt}, completion:{current_completion}, total:{current_total}] ‚Üí Running Total: {_token_accounting['total_tokens']}\"\n",
    "    elif current_total > 0:\n",
    "        # Edge case: has tokens but not marked as LLM call (investigation needed)\n",
    "        line3 = f\"‚ö†Ô∏è Tokens: {current_total} (has tokens but not marked as LLM call) ‚Üí Running Total: {_token_accounting['total_tokens']}\"\n",
    "    else:\n",
    "        # No tokens (normal for non-LLM events)\n",
    "        line3 = f\"Tokens: None ‚Üí Running Total: {_token_accounting['total_tokens']}\"\n",
    "\n",
    "    line4 = f\"Timestamp: {timestamp} | Event ID: {event_id}\"\n",
    "\n",
    "    # Format the log output with enhanced visual structure\n",
    "    separator_line = \"‚îÅ\" * 80\n",
    "    content_preview = str(getattr(event, \"content\", str(event)))\n",
    "\n",
    "    # Truncate very long content for readability\n",
    "    # if len(content_preview) > 500:\n",
    "    #     content_preview = (\n",
    "    #         content_preview[:500] + \"\\n... [Content truncated for display] ...\"\n",
    "    #     )\n",
    "\n",
    "    logger.info(\n",
    "        f\"\"\"\n",
    "{separator_line}\n",
    "{line1}\n",
    "{line2}\n",
    "{line3}\n",
    "{line4}\n",
    "{separator_line}\n",
    "{content_preview}\n",
    "{separator_line}\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return is_llm_call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SYSTEM PROMPTS\n",
    "\n",
    "### References\n",
    "# https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices\n",
    "# https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/prompt-engineering-for-openai%E2%80%99s-o1-and-o3-mini-reasoning-models/4374010\n",
    "# https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf\n",
    "today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "PLANNER_SYSTEM_PROMPT = dedent(\n",
    "    f\"\"\"Strategic Research Planner. Convert user queries into executable research specifications. Today is {today_str}.\n",
    "\n",
    "CORE FUNCTION: Convert user queries into executable research specifications.\n",
    "- Decompose queries into specific, answerable research questions.\n",
    "- Set evidence thresholds: Established facts (1-2 sources), trends (3+ sources), contested claims (5+ sources)\n",
    "- Define temporal scope and boundaries\n",
    "- PLANNING ONLY: No research execution or evidence assessment\n",
    "\n",
    "PLAN FORMAT:\n",
    "```\n",
    "## Research Brief: [Title]\n",
    "### Questions: [3-5 specific questions]\n",
    "### Evidence: [source thresholds per question] \n",
    "### Scope: [temporal/geographic boundaries]\n",
    "### Deliverable: [report type]\n",
    "```\n",
    "\n",
    "OUTPUT SIGNALS:\n",
    "- \"PLAN_CREATED ‚Üí @Critic\" (initial)\n",
    "- \"PLAN_REVISED ‚Üí @Critic\" (after revision)  \n",
    "- \"PLAN_APPROVED\" (after critic approval)\n",
    "- \"PLAN_ABANDONED: [reason]\" (after 5 attempts)\n",
    "\n",
    "COMPLETION SIGNALS:\n",
    "- Wait for explicit CRITIC approval before declaring \"PLAN_APPROVED\"\n",
    "- Never hand off to RESEARCHER without CRITIC approval\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "RESEARCHER_SYSTEM_PROMPT = dedent(\n",
    "    f\"\"\"Senior Research Analyst. Execute targeted research using available tools with strict resource management. Today is {today_str}.\n",
    "\n",
    "CORE FUNCTION:\n",
    "- Execute research questions, gather evidence meeting specified thresholds\n",
    "- Gather evidence, respecting the specified source thresholds and search heuristics.\n",
    "\n",
    "OOL DECISION TREE:\n",
    "- HISTORICAL/DEFINITION/ESTABLISHED FACT needs ‚Üí wiki_search only\n",
    "   Example: \"What is\", \"History of\", \"Definition\"\n",
    "- CURRENT DATA/RECENT TRENDS/MARKET DATA needs ‚Üí web_search first\n",
    "   Example: Contains \"2024\", \"2025\", \"current\", \"latest\", \"recent\"\n",
    "- DETAILED CONTENT gaps ‚Üí web_fetch (max 5 URLs per query)\n",
    "   Only if web_search gives <500 words total content\n",
    "\n",
    "SEARCH HEURISTICS:\n",
    "- Include year for current topics: \"AI adoption 2025\"\n",
    "- Geographic: \"EV sales Europe 2025\"\n",
    "- For contested topics, prioritize diverse source perspectives over source volume\n",
    "- Source priority: Primary > Institutional > Peer-reviewed > News\n",
    "\n",
    "EFFICIENCY RULES:\n",
    "- SUFFICIENT_EVIDENCE: Stop when confidence threshold reached\n",
    "- DIMINISHING_RETURNS: Automatically terminate if <20% new info across 2 searches\n",
    "- CIRCULAR_DETECTION: Automatically terminate if 3+ searches share >70% term overlap.\n",
    "- QUALITY_OVER_QUANTITY: 3 high-quality sources > 10 weak sources\n",
    "\n",
    "OUTPUT:\n",
    "- Group findings by research theme\n",
    "- Include source URLs and publication dates\n",
    "- Flag incomplete evidence areas\n",
    "- NO quality assessment (EDITOR's role)\n",
    "\n",
    "COMPLETION SIGNALS:\n",
    "- Evidence thresholds met ‚Üí \"RESEARCH_COMPLETE ‚Üí @Critic\"\n",
    "- RESEARCH_APPROVED (after critic approval)\n",
    "- Data inaccessible ‚Üí \"RESEARCH_BLOCKED: [reason]\"\n",
    "- DIMINISHING_RETURNS (new info threshold not met)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "CRITIC_SYSTEM_PROMPT = dedent(\n",
    "    f\"\"\"Domain-agnostic Critic. Provide focused feasibility and completeness review when explicitly called. Today is {today_str}.\n",
    "\n",
    "CORE FUNCTION:\n",
    "- PLAN_REVIEW: Assess feasibility, scope clarity, tool alignment, temporal boundaries. Feedback to PLANNER\n",
    "- ARTIFACT_REVIEW OR EDIT_REVIEW: Check completeness, clarity, strategic value, temporal accuracy. Feedback to EDITOR\n",
    "- NO RESEARCH execution\n",
    "\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "- \"APPROVED\" (ready for next stage)\n",
    "- \"REJECTED: [specific issues] ‚Üí [required changes]\"\n",
    "\n",
    "QUALITY LABELS (for all reviews):\n",
    "- HIGH: Multiple credible sources, robust methodology\n",
    "- MEDIUM: Single credible source or minor limitations\n",
    "- LOW: Preliminary evidence or significant gaps\n",
    "- INSUFFICIENT: Below minimum threshold for meaningful analysis\n",
    "\n",
    "REJECTION TRIGGERS:\n",
    "- Requires specialized databases/paid access\n",
    "- Vague success criteria or unlimited resource requirements  \n",
    "- Questions beyond search/fetch tool capabilities\n",
    "- Evidence insufficient for stated confidence claims\n",
    "\n",
    "RESOURCE LIMITS:\n",
    "- MAX_RESPONSE: 300 words\n",
    "- REVIEW_ONLY: No unsolicited feedback\"\"\"\n",
    ")\n",
    "\n",
    "EDITOR_SYSTEM_PROMPT = dedent(\n",
    "    \"\"\"Research Editorial Agent. Transform research into well-structured, actionable reports enabling strategic application.Today is {today_str}.\n",
    "\n",
    "CORE ROLE:\n",
    "- Synthesize researcher findings. \n",
    "- Generate strategic insights bounded by evidence quality. Clarity over comprehensiveness\n",
    "- Acknowledge data limitations transparently bounded by confidence indicators to all claims\n",
    "- Save final report using save_report tool\n",
    "\n",
    "EVIDENCE STANDARDS:\n",
    "- High confidence: Multiple credible sources, recent data\n",
    "- Medium confidence: Single credible source or methodological limits\n",
    "- Low confidence: Preliminary/conflicting evidence, significant gaps\n",
    "- NO INSIGHTS beyond evidence boundaries\n",
    "\n",
    "ADAPTIVE FORMATTING: Adapt structure to maximize value delivery\n",
    "- TREND_ANALYSIS: Executive Summary + Trend Analysis + Future Outlook + Sources  \n",
    "- MARKET_RESEARCH: Executive Summary + Market Overview + Key Players + Recommendations + Sources\n",
    "- COMPARATIVE_STUDY: Executive Summary + Comparison Matrix + Analysis + Sources\n",
    "- BUSINESS_ANALYSIS: Executive Summary + Business Overview + Market Analysis + Competitive Landscape + Financial Analysis + Sources\n",
    "- TECHNOLOGY_ASSESSMENT: Executive Summary + Technology Overview + Adoption Trends + Implementation Challenges + Sources\n",
    "- SECTORAL_ANALYSIS: Executive Summary + Sector Overview + Key Players + Trends + Sources\n",
    "- COMPARATIVE_ANALYSIS: Executive Summary + Comparison Framework + Key Insights + Sources\n",
    "\n",
    "COMPLETION TRIGGERS:\n",
    "1. Save report using save_report tool\n",
    "2. Output: \"REPORT_SAVED. TASK_COMPLETE. TERMINATE\"\n",
    "3. MANDATORY: After save_report, respond only with 'REPORT_SAVED. TASK_COMPLETE. TERMINATE' and provide no additional content\"\n",
    "\n",
    "FAILURE MODES:\n",
    "- CRITIC_REJECTION: Report lacks evidence support. Proceed with qualified report after MAX_CRITIC_INTERACTIONS\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGENT SETUP\n",
    "\n",
    "\n",
    "planner = AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    model_client=gemini_model_client,\n",
    "    system_message=PLANNER_SYSTEM_PROMPT,\n",
    "    description=\"Creates and adapts research plans, handles replanning when research hits obstacles\",\n",
    ")\n",
    "\n",
    "researcher = AssistantAgent(\n",
    "    name=\"Researcher\",\n",
    "    description=\"Expert research agent that strategically uses multiple tools to gather comprehensive and factual evidence to produce well-researched draft reports.\",\n",
    "    model_client=gemini_model_client,\n",
    "    tools=[wiki_search_tool, web_search_tool, web_fetch_tool, web_fetch_multiple_tool],\n",
    "    reflect_on_tool_use=False,\n",
    "    system_message=RESEARCHER_SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "critic = AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    model_client=gemini_model_client,\n",
    "    system_message=CRITIC_SYSTEM_PROMPT,\n",
    "    description=\"Reviews and provides constructive criticism; outputs 'APPROVED: [...]' when ready.\",\n",
    ")\n",
    "\n",
    "\n",
    "editor = AssistantAgent(\n",
    "    name=\"Editor\",\n",
    "    description=\"Formats approved drafts with proper citations, adapts structure to content type.\",\n",
    "    model_client=gemini_model_client,\n",
    "    tools=[save_report_tool],\n",
    "    system_message=EDITOR_SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "__SELECTOR_PROMPT = \"\"\"Smart agent selector with task progression awareness.\n",
    "\n",
    "AGENT CAPABILITIES:\n",
    "{roles}\n",
    "\n",
    "Available agents: \n",
    "{participants}\n",
    "\n",
    "Conversation history: \n",
    "{history}\n",
    "\n",
    "QUALITY CONTROL FLOW:\n",
    "1. Query ‚Üí Planner (creates plan)\n",
    "2. Plan ‚Üí Critic (feasibility review) \n",
    "3. Approved Plan ‚Üí Researcher (execution)\n",
    "4. Research ‚Üí Critic (quality review)\n",
    "5. Approved Research ‚Üí Editor (final report)\n",
    "\n",
    "ROUTING LOGIC:\n",
    "- No plan ‚Üí Planner\n",
    "- \"PLAN_CREATED\" or \"PLAN_REVISED\" ‚Üí Critic\n",
    "- \"PLAN_APPROVED\" ‚Üí Researcher\n",
    "- Research complete ‚Üí Critic\n",
    "- \"RESEARCH_APPROVED\" ‚Üí Editor\n",
    "- \"REPORT_SAVED\" ‚Üí TERMINATE\n",
    "\n",
    "QUALITY GATES: All plans and research require CRITIC approval\n",
    "\n",
    "Return only the next agent name.\"\"\"\n",
    "\n",
    "\n",
    "# Team configuration with constants\n",
    "max_messages = TASK_TERMINATION_MAX_MESSAGES\n",
    "txt_termination = TextMentionTermination(\"TERMINATE\")\n",
    "termination_condition = (\n",
    "    MaxMessageTermination(max_messages=max_messages) | txt_termination\n",
    ")\n",
    "\n",
    "# Build SelectorGroupChat\n",
    "\n",
    "model_context = BufferedChatCompletionContext(buffer_size=CONVERSATION_BUFFER_SIZE)\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    name=\"Deep Research Team\",\n",
    "    description=\"A team of specialized agents working together to conduct deep research.\",\n",
    "    model_context=model_context,\n",
    "    participants=[planner, researcher, critic, editor],\n",
    "    model_client=gemini_model_client,\n",
    "    selector_prompt=__SELECTOR_PROMPT,\n",
    "    termination_condition=termination_condition,\n",
    "    emit_team_events=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP TASK RUN\n",
    "\n",
    "\n",
    "# @tenacity.retry(\n",
    "#     wait=tenacity.wait_exponential(multiplier=1, min=60, max=120),\n",
    "#     stop=tenacity.stop_after_attempt(2),\n",
    "#     retry=tenacity.retry_if_exception_type(Exception),\n",
    "# )\n",
    "async def run_task(task_text: str):\n",
    "    \"\"\"\n",
    "    Execute a multi-agent research task with robust termination handling.\n",
    "\n",
    "    Clean and simple - all token accounting is handled by log_event_enhanced().\n",
    "    Relies on explicit agent termination signals without backup conditionals.\n",
    "\n",
    "    Args:\n",
    "        task_text (str): The research task description\n",
    "\n",
    "    Returns:\n",
    "        str | None: Final report content or None if task incomplete\n",
    "    \"\"\"\n",
    "    final_report = None\n",
    "    task_completed = False\n",
    "\n",
    "    # Simple local variables\n",
    "    event_count = 0\n",
    "    tool_call_count = 0\n",
    "\n",
    "    # Reset token accounting for new task\n",
    "    reset_token_accounting()\n",
    "\n",
    "    # Streamlined termination signals - agents handle termination explicitly\n",
    "    termination_signals = [\n",
    "        \"TERMINATE\",\n",
    "        \"REPORT_SAVED\",\n",
    "        \"TASK_COMPLETE\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"üöÄ Starting task: {task_text}\\n\\n\")\n",
    "\n",
    "        async for event in team.run_stream(task=task_text):\n",
    "\n",
    "            # Increment total event counter\n",
    "            event_count += 1\n",
    "\n",
    "            # Centralized logging with token accounting - returns if this was an LLM call\n",
    "            is_llm_call = log_event_enhanced(event, event_count)\n",
    "\n",
    "            # Track tool calls for statistics only\n",
    "            if hasattr(event, \"tool_calls\") and event.tool_calls:\n",
    "                tool_call_count += len(event.tool_calls)\n",
    "\n",
    "            # Check for termination signals in message content\n",
    "            if hasattr(event, \"content\") and isinstance(event.content, str):\n",
    "                content_upper = event.content.upper()\n",
    "\n",
    "                # Check for any termination signal\n",
    "                for signal in termination_signals:\n",
    "                    if signal.upper() in content_upper:\n",
    "                        final_report = event.content.split(signal, 1)[0].strip()\n",
    "                        task_completed = True\n",
    "\n",
    "                        # Get final stats from token accounting\n",
    "                        stats = get_token_stats()\n",
    "                        logger.info(\n",
    "                            f\"\\n\\n‚úÖ Task completed with '{signal}' signal after {event_count} total events ({stats['llm_call_count']} LLM calls)\"\n",
    "                        )\n",
    "                        logger.info(\n",
    "                            f\"üìä Final Token Usage - Total: {stats['total_tokens']} (prompt: {stats['total_prompt_tokens']}, completion: {stats['total_completion_tokens']})\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                if task_completed:\n",
    "                    break\n",
    "\n",
    "            # Apply rate limiting based on event type\n",
    "            if is_llm_call:\n",
    "                stats = get_token_stats()\n",
    "                logger.info(\n",
    "                    f\"\\n\\n‚è≥ LLM API call #{stats['llm_call_count']} detected, waiting {API_CALL_DELAY_SECONDS} seconds. (Total events: {event_count})\\n\\n\"\n",
    "                )\n",
    "                await asyncio.sleep(API_CALL_DELAY_SECONDS)\n",
    "            else:\n",
    "                # Non-API event (planning, internal processing) - minimal delay\n",
    "                logger.debug(\n",
    "                    f\"\\n\\nüí® Non-LLM event #{event_count}, brief pause ({NON_API_EVENT_DELAY}s)\\n\\n\"\n",
    "                )\n",
    "                await asyncio.sleep(NON_API_EVENT_DELAY)\n",
    "\n",
    "        # Log final totals and statistics\n",
    "        if not task_completed:\n",
    "            stats = get_token_stats()\n",
    "            logger.warning(\n",
    "                f\"‚ö†Ô∏è Task reached end of stream without clear termination after {event_count} events ({stats['llm_call_count']} LLM calls)\"\n",
    "            )\n",
    "\n",
    "        # Final statistics logging\n",
    "        stats = get_token_stats()\n",
    "        logger.info(\n",
    "            f\"\"\"\n",
    "üìä TASK COMPLETION STATISTICS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ Task Completed: {task_completed}\n",
    " Total Events: {event_count}\n",
    "ü§ñ LLM API Calls: {stats['llm_call_count']}\n",
    "üîß Tool Calls: {tool_call_count}\n",
    " Tokens Used: {stats['total_tokens']} (prompt: {stats['total_prompt_tokens']}, completion: {stats['total_completion_tokens']})\n",
    "üîÑ Processed Unique Events: {len(stats['processed_event_ids'])}\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        return final_report if task_completed else None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\n\\n‚ùå Exception during task execution: {e}\")\n",
    "        stats = get_token_stats()\n",
    "        logger.info(\n",
    "            f\"üìä Error at Event #{event_count} (LLM calls: {stats['llm_call_count']}), Token Usage: {stats['total_tokens']} (prompt: {stats['total_prompt_tokens']}, completion: {stats['total_completion_tokens']})\"\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        await _save_team_state(task_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb513d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN TASK\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    __SAMPLE_QUERY = queries = [\n",
    "        \"Indian steel sector growth post modernization and growth prospects in an era of US tariffs and reduce government protection through trade barriers and cheaper import options from China\",\n",
    "        \"How government and governance factors improved economy and lives of indians during Modi and Pre-Modi starting from 1991\",\n",
    "        \"Sectoral growth based on cyclics for 2025 and macro economic pressure and trade tariffs and uncertainty, which sectors are best poised for maximum investment returns in terms of % for the next year for a moderate to average risk profile investments\",\n",
    "        \"Hyperscaler investments in data centers and cloud infrastructure for AI growth is not matching the proposed productivity gains in GDP. Are we witnessing a bubble? Is % of global GDP being invested in AI infrastructure matches the productivity gain percentages?\",\n",
    "        \"If neural networks are foundation of LLMs and based on the human brain; Are LLMs given tools during training? Humans learn with tool usage, What are current trends on tool usage in LLM training?\",\n",
    "        f\"Today is {today_str}. Analyze stock price performance of Nvidia in the past month, compare it with top 3 listed POWER Producers in India.\",\n",
    "        \"Evaluating the long-term viability of green hydrogen as a baseload power source in India given current electrolyzer costs, renewable energy tariffs, grid stability constraints, and projected policy support through 2035\",\n",
    "        \"Assessing whether India‚Äôs 2025-30 urban housing shortage can be resolved through large-scale 3D-printed construction without triggering systemic risk in NBFC and banking balance sheets exposed to real estate credit\",\n",
    "        \"Quantifying the impact of EU CBAM (Carbon Border Adjustment Mechanism) on India‚Äôs MSME-dominated textile export clusters in Tiruppur and Surat, including cascading effects on informal employment and regional GDP\",\n",
    "        \"Mapping supply-chain chokepoints for critical rare-earth elements (Neodymium, Dysprosium) essential for India‚Äôs EV and renewable energy targets, and evaluating geopolitical fallback strategies if China restricts exports\",\n",
    "        \"Determining whether India‚Äôs Unified Payments Interface (UPI) can scale to serve as the backbone for a sovereign digital currency (CBDC) while preserving offline transaction capability and financial inclusion in rural hinterlands\",\n",
    "        \"Analyzing if the projected 2025-30 growth in India‚Äôs domestic semiconductor consumption can justify the capital intensity of new fabs without sustained government subsidies and tax incentives that crowd out social-sector spending\",\n",
    "        \"Investigating whether India‚Äôs demographic dividend can offset the fiscal drag from rising health-care costs driven by lifestyle diseases, by modeling the combined effect of PM-JAY coverage expansion and private insurance penetration\",\n",
    "        \"Examining the systemic risk posed to Indian mutual funds and pension portfolios from concentrated exposure to Adani Group entities under evolving ESG disclosure norms and potential climate-litigation scenarios\",\n",
    "        \"Evaluating the comparative efficiency of India‚Äôs inland waterways versus dedicated freight corridors in reducing logistics costs for bulk commodities, while accounting for seasonal monsoon disruptions and inter-state regulatory friction\",\n",
    "        \"Determining if open-source foundation-model ecosystems (e.g., BLOOM, LLaMA derivatives) can reduce India‚Äôs reliance on proprietary LLM APIs, and measuring the incremental TCO of sovereign GPU clusters versus foreign cloud dependency\",\n",
    "    ]\n",
    "    try:\n",
    "\n",
    "        output = asyncio.run(run_task(__SAMPLE_QUERY[6]))\n",
    "        if output:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"üéØ FINAL REPORT\")\n",
    "            print(\"=\" * 60)\n",
    "            print(output)\n",
    "        else:\n",
    "            print(\"‚ùå Task did not complete successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Fatal error: {e}\")\n",
    "        print(f\"‚ùå Execution failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEAM STATE MANAGEMENT EXAMPLES\n",
    "\n",
    "# Example: List all available team state files\n",
    "print(\"üìã Available Team State Files:\")\n",
    "print(\"=\" * 50)\n",
    "file_info = list_team_state_files()\n",
    "\n",
    "# Example: Resume from most recent state file (auto-detect)\n",
    "# await resume_from_saved_state()\n",
    "\n",
    "# Example: Resume from specific state file\n",
    "# await resume_from_saved_state(\"20250824_1430_green_hydrogen_viability_team_state.json\")\n",
    "\n",
    "\n",
    "# Example: Resume with error handling\n",
    "async def safe_resume(state_file: str = None):\n",
    "    try:\n",
    "        await resume_from_saved_state(state_file)\n",
    "        logger.info(\"‚úÖ Successfully resumed from saved state\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to resume: {e}\")\n",
    "\n",
    "\n",
    "# Uncomment to test:\n",
    "# await safe_resume()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ef532",
   "metadata": {},
   "source": [
    "> **Note:** All values are approximate and reflect estimates as of **August 2025**. Word, token, sentence, and character counts can vary by writing style and tokenization method.\n",
    "\n",
    "- Tokens calculated as `words √∑ 0.75`.\n",
    "- Sentences calculated as `words √∑ 15‚Äì20`.\n",
    "- Characters calculated as `words √ó 4‚Äì5`.\n",
    "\n",
    "| Article Type / Measure         | Approx Words | Approx Sentences | Approx Tokens | Approx Characters | Notes                               | Suggested Summary (Tokens / Chars)     |\n",
    "| ------------------------------ | ------------ | ---------------- | ------------- | ----------------- | ----------------------------------- | -------------------------------------- |\n",
    "| Short text (60 tokens)         | ~45          | ~2‚Äì3             | ~60           | ~180‚Äì225          | Short paragraph / social media post | ~10 tokens / ~40‚Äì50 chars              |\n",
    "| Blog article                   | 1,000‚Äì1,800  | ~50‚Äì120          | ~1,300‚Äì2,400  | ~4,000‚Äì9,000      | Typical online blog length          | ~150‚Äì360 tokens / ~400‚Äì900 chars       |\n",
    "| NYT Op-Ed                      | 800‚Äì1,200    | ~40‚Äì80           | ~1,050‚Äì1,600  | ~3,200‚Äì6,000      | Opinion/editorial piece             | ~100‚Äì240 tokens / ~320‚Äì600 chars       |\n",
    "| Research article (web/journal) | 3,000‚Äì7,000  | ~150‚Äì470         | ~4,000‚Äì9,300  | ~12,000‚Äì35,000    | Standard journal or web publication | ~400‚Äì1,400 tokens / ~1,200‚Äì3,500 chars |\n",
    "| arXiv preprint                 | 6,000‚Äì12,000 | ~300‚Äì800         | ~8,000‚Äì16,000 | ~24,000‚Äì60,000    | Preprint scientific paper           | ~800‚Äì1,600 tokens / ~2,400‚Äì6,000 chars |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
